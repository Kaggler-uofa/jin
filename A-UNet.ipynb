{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a88eee",
   "metadata": {
    "id": "yrJ6vqVJel3F",
    "papermill": {
     "duration": 0.018381,
     "end_time": "2024-02-06T22:59:42.968054",
     "exception": false,
     "start_time": "2024-02-06T22:59:42.949673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Kaggler: SenNet + HOA - Hacking the Human Vasculature in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ddabd",
   "metadata": {
    "id": "kFwnqnUweoyp",
    "papermill": {
     "duration": 0.017762,
     "end_time": "2024-02-06T22:59:43.004406",
     "exception": false,
     "start_time": "2024-02-06T22:59:42.986644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "<b>Team:</b> Kaggler <br>\n",
    "<b>Team Members:</b> Hyunji Cha, Minjae Jeong, Min Joh, Jin You <br><br>\n",
    "\n",
    "> <b>Competition Information:</b><br>\n",
    "Title: SenNet + HOA - Hacking the Human Vasculature in 3D <br>\n",
    "Host: SenNet + HOA <br>\n",
    "Platform: Kaggle <br>\n",
    "Final Submission Deadline: February 6, 2024 <br>\n",
    "Link: https://www.kaggle.com/competitions/blood-vessel-segmentation <br>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df028977",
   "metadata": {
    "id": "6RRibNl9dwr3",
    "papermill": {
     "duration": 0.018338,
     "end_time": "2024-02-06T22:59:43.040805",
     "exception": false,
     "start_time": "2024-02-06T22:59:43.022467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0. Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b56d4c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:43.077288Z",
     "iopub.status.busy": "2024-02-06T22:59:43.076941Z",
     "iopub.status.idle": "2024-02-06T22:59:51.363118Z",
     "shell.execute_reply": "2024-02-06T22:59:51.362192Z"
    },
    "executionInfo": {
     "elapsed": 6047,
     "status": "ok",
     "timestamp": 1706587641493,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "HQw9k5Hwdwr5",
    "papermill": {
     "duration": 8.307422,
     "end_time": "2024-02-06T22:59:51.365647",
     "exception": false,
     "start_time": "2024-02-06T22:59:43.058225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, transform, exposure\n",
    "from tqdm import tqdm\n",
    "import albumentations as Alb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd322d",
   "metadata": {
    "id": "erPOxPE0dwr6",
    "papermill": {
     "duration": 0.019697,
     "end_time": "2024-02-06T22:59:51.405347",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.385650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca85847b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:51.445072Z",
     "iopub.status.busy": "2024-02-06T22:59:51.444189Z",
     "iopub.status.idle": "2024-02-06T22:59:51.537891Z",
     "shell.execute_reply": "2024-02-06T22:59:51.536916Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1706587641494,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "oIuI0NW8dwr6",
    "papermill": {
     "duration": 0.115888,
     "end_time": "2024-02-06T22:59:51.539994",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.424106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \"\"\"\n",
    "    Configuration class for the blood vessel segmentation project.\n",
    "    \"\"\"\n",
    "    # ============== Data Paths =============\n",
    "    DATASET_DIRECTORY = os.path.abspath(os.path.join(\"..\", \"..\", \"kaggle\", \"input\", \"blood-vessel-segmentation\"))\n",
    "    TRAIN_DATASET_NAME = \"kidney_1_dense\"\n",
    "    # DATASET_DIRECTORY = os.path.abspath(os.path.join(\"..\", \"blood-vessel-segmentation\"))\n",
    "    TRAIN_DATASET_DIRECTORY = os.path.join(DATASET_DIRECTORY, \"train\", TRAIN_DATASET_NAME)\n",
    "    TEST_DATASET_NAME = \"kidney_2\"\n",
    "    TEST_DATASET_DIRECTORY = os.path.join(DATASET_DIRECTORY, \"train\", TEST_DATASET_NAME)\n",
    "\n",
    "    # ============== Model Configuration =============\n",
    "    MODEL_NAME = 'AUNet'\n",
    "    IN_CHANNEL = 1  # Number of input channels (e.g., 1 for grayscale images, 3 for RGB images)\n",
    "    OUT_CHANNEL = 1 # Number of output channels\n",
    "\n",
    "    # ============== Model Paths =============\n",
    "    #CHECKPOINT_TEST_PATH = os.path.abspath(\"..\")\n",
    "    # List of paths to trained model weights\n",
    "    CHECKPOINT_PATH = os.path.abspath(os.path.join(\"..\", \"..\", \"kaggle\", \"working\", \"checkpoints\"))\n",
    "    TRAINED_MODEL = f\"{MODEL_NAME}_checkpoint.pth\"\n",
    "    #TRAINED_MODEL = f\"{MODEL_NAME}_kidney_1_dense_checkpoint.pth\"\n",
    "    #/kaggle/input/attention-unet/pytorch/real/{version}\n",
    "    MODEL_PATH = os.path.abspath(os.path.join(\"..\", \"..\", \"kaggle\", \"input\", \"attention-unet\", \"pytorch\", \"real\", \"1\", \"checkpoints\"))\n",
    "    #MODEL_PATH = None\n",
    "\n",
    "    # ============== Image Processing Settings =============\n",
    "    INPUT_IMAGE_SIZE = (512, 512)  # Size of the input images (height x width)\n",
    "    GAMMA = 1\n",
    "    NUM_IMAGES = None # 'None' for all images\n",
    "    BINARY_CHECK = False\n",
    "\n",
    "    # ============== Training and Validation Parameters =============\n",
    "    IF_TRAIN = False\n",
    "    IF_TEST = False\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #VALID_ID = 1  # ID for validation dataset or fold (e.g., 1 first fold is being used as validation set)\n",
    "    BATCH_SIZE = 1  # Batch size for model training\n",
    "    THRESHOLD_PERCENTILE = 0.0014109  # Threshold for post-processing\n",
    "    LEARNING_RATE = 1e-4\n",
    "    NUM_EPOCHS = 30\n",
    "    NUM_WORKERS = 4 # Number of processes\n",
    "    TRAIN_TEST_SPLIT_RATIO = 0.2 # for test subset\n",
    "    \n",
    "    # ============== Training and Validation Parameters =============\n",
    "    IF_SUBMISSION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1301b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:51.579091Z",
     "iopub.status.busy": "2024-02-06T22:59:51.578702Z",
     "iopub.status.idle": "2024-02-06T22:59:51.589408Z",
     "shell.execute_reply": "2024-02-06T22:59:51.588257Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1706587641494,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "1kZkWLg7JFdQ",
    "outputId": "7fad5938-7dda-4f2b-f87d-8d4eaefcd06a",
    "papermill": {
     "duration": 0.032773,
     "end_time": "2024-02-06T22:59:51.591561",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.558788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a0fff",
   "metadata": {
    "id": "vjFUFjOddwr6",
    "papermill": {
     "duration": 0.018734,
     "end_time": "2024-02-06T22:59:51.629422",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.610688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7e8fa",
   "metadata": {
    "id": "u1-Cb8Ajdwr7",
    "papermill": {
     "duration": 0.019519,
     "end_time": "2024-02-06T22:59:51.668412",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.648893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a5bf1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:51.709083Z",
     "iopub.status.busy": "2024-02-06T22:59:51.708689Z",
     "iopub.status.idle": "2024-02-06T22:59:51.733748Z",
     "shell.execute_reply": "2024-02-06T22:59:51.732374Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1706587641494,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "RkdW3SSydwr7",
    "papermill": {
     "duration": 0.048717,
     "end_time": "2024-02-06T22:59:51.736642",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.687925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path=None, target_image_size=(512, 512), gamma=None, image_format=\".tif\"):\n",
    "        # File IO\n",
    "        self.gamma = gamma if gamma is not None else 1\n",
    "        self.image_path = image_path\n",
    "        self.image_format = image_format\n",
    "        self.image_filenames = sorted([os.path.join(self.image_path, filename) for filename in os.listdir(self.image_path) if filename.endswith(self.image_format)])\n",
    "        print(f\"{len(self.image_filenames)} images are loaded.\")\n",
    "\n",
    "        # If masks (labels) are given\n",
    "        self.mask_path = mask_path\n",
    "        if self.mask_path:\n",
    "            self.mask_filenames = sorted([os.path.join(self.mask_path, filename) for filename in os.listdir(self.mask_path) if filename.endswith(self.image_format)])\n",
    "            if not len(self.mask_filenames) == len(self.image_filenames):\n",
    "                self.image_filenames, self.mask_filenames = self.match_images_and_labels(self.image_filenames, self.mask_filenames)\n",
    "            print(f\"{len(self.mask_filenames)} labels are loaded.\")\n",
    "\n",
    "        # Image processing\n",
    "        self.original_image_size = None\n",
    "        self.target_image_size = target_image_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Process and print the size of the first image if convert_size is True\n",
    "        if self.target_image_size and len(self.image_filenames) > 0:\n",
    "            first_image = io.imread(self.image_filenames[0])\n",
    "            resized_image = self.resize_and_pad(first_image, self.target_image_size)\n",
    "            self.original_image_size = first_image.shape\n",
    "            print(f\"Original size: {self.original_image_size}, Converted size: {resized_image.shape}\")\n",
    "\n",
    "    def match_images_and_labels(self, image_filenames, mask_filenames):\n",
    "      matched_images = []\n",
    "      matched_labels = []\n",
    "\n",
    "      image_basenames = {os.path.basename(path).split('.')[0] for path in image_filenames}\n",
    "      label_basenames = {os.path.basename(path).split('.')[0] for path in mask_filenames}\n",
    "\n",
    "      common_basenames = image_basenames.intersection(label_basenames)\n",
    "\n",
    "      for basename in common_basenames:\n",
    "        matched_images.append(os.path.join(self.image_path, basename + self.image_format))\n",
    "        matched_labels.append(os.path.join(self.mask_path, basename + self.image_format))\n",
    "\n",
    "      return sorted(matched_images), sorted(matched_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image and convert to float32\n",
    "        # imread returns 2d numpy array (grayscale) and 3d numpy array (h,w,c) (rgb)\n",
    "        image = io.imread(self.image_filenames[index]).astype(np.float32)\n",
    "\n",
    "        # Resize and pad the image if convert_size is True\n",
    "        if self.target_image_size:\n",
    "            image = self.resize_and_pad(image, self.target_image_size, self.gamma)\n",
    "\n",
    "        # Extract the filename\n",
    "        # os.path.basename extract the base name (i.e., the final part) of a pathname\n",
    "        image_filename = os.path.basename(self.image_filenames[index])\n",
    "\n",
    "        # If masks (labels) are given\n",
    "        if self.mask_path:\n",
    "            mask = io.imread(self.mask_filenames[index]).astype(np.uint8) # np.uint8 is used for binary masks due to its memory efficiency\n",
    "            mask = self.resize_and_pad(mask, self.target_image_size, is_mask=True)\n",
    "\n",
    "            # Convert mask from [0, 255] to [0, 1]\n",
    "            mask = (mask > 0).astype(np.uint8)  # This converts all non-zero values to 1\n",
    "\n",
    "            return image, mask, image_filename\n",
    "\n",
    "        return image, image_filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def resize_and_pad(self, image, target_image_size, is_mask=False):\n",
    "\n",
    "        gamma = self.gamma\n",
    "\n",
    "        \"\"\"Resize an image or mask and add padding to keep aspect ratio.\"\"\"\n",
    "        # Calculate scale and padding\n",
    "        h, w = image.shape[:2]\n",
    "        scale = min(target_image_size[0] / h, target_image_size[1] / w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        pad_h = (target_image_size[0] - new_h) // 2\n",
    "        pad_w = (target_image_size[1] - new_w) // 2\n",
    "\n",
    "        # Set parameters for resizing based on whether it's an image or a mask\n",
    "        if is_mask:\n",
    "            # Use nearest-neighbor interpolation for masks\n",
    "            order = 0\n",
    "            anti_aliasing = False\n",
    "        else:\n",
    "            # Use bilinear interpolation (order=1) for images\n",
    "            order = 1\n",
    "            anti_aliasing = True # minimize visual distortions when reducing the image size; averaging the colors of the pixels at the edges of contrasting areas (blurs edges)\n",
    "\n",
    "        # Resize image with preserve_range set to True\n",
    "        image_resized = transform.resize(\n",
    "            image,\n",
    "            (new_h, new_w),\n",
    "            anti_aliasing=anti_aliasing,\n",
    "            mode='constant',\n",
    "            preserve_range=True,  # Preserve the original image's intensity range\n",
    "            order=order\n",
    "        )\n",
    "\n",
    "        # Apply gamma correction to images only\n",
    "        if not is_mask and gamma != 1:\n",
    "            image_resized = exposure.adjust_gamma(image_resized, gamma)\n",
    "\n",
    "        # Add a channel dimension to grayscale images if necessary\n",
    "        if len(image.shape) == 2:  # Grayscale image\n",
    "            image_resized = image_resized[..., np.newaxis]\n",
    "\n",
    "        # Initialize padded image\n",
    "        padded_image = np.zeros((target_image_size[0], target_image_size[1], image_resized.shape[2]), dtype=image_resized.dtype)\n",
    "\n",
    "        # Insert the resized image into the padded image\n",
    "        padded_image[pad_h:pad_h+new_h, pad_w:pad_w+new_w, :] = image_resized\n",
    "\n",
    "        return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2266c12f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:51.776324Z",
     "iopub.status.busy": "2024-02-06T22:59:51.775590Z",
     "iopub.status.idle": "2024-02-06T22:59:51.790859Z",
     "shell.execute_reply": "2024-02-06T22:59:51.789945Z"
    },
    "papermill": {
     "duration": 0.037033,
     "end_time": "2024-02-06T22:59:51.792851",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.755818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PipelineDataset(Dataset):\n",
    "    def __init__(self, image_filenames, target_image_size=(512, 512), gamma=None, image_format=\".tif\"):\n",
    "        # File IO\n",
    "        self.gamma = gamma if gamma is not None else 1\n",
    "        self.image_format = image_format\n",
    "        \n",
    "        # if filenames are given\n",
    "        self.image_filenames = image_filenames\n",
    "\n",
    "        # Image processing\n",
    "        self.original_image_size = None\n",
    "        self.target_image_size = target_image_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Process and print the size of the first image if convert_size is True\n",
    "        if self.target_image_size and len(self.image_filenames) > 0:\n",
    "            first_image = io.imread(self.image_filenames[0])\n",
    "            resized_image = self.resize_and_pad(first_image, self.target_image_size)\n",
    "            self.original_image_size = first_image.shape\n",
    "            print(f\"Original size: {self.original_image_size}, Converted size: {resized_image.shape}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image and convert to float32\n",
    "        # imread returns 2d numpy array (grayscale) and 3d numpy array (h,w,c) (rgb)\n",
    "        image = io.imread(self.image_filenames[index]).astype(np.float32)\n",
    "\n",
    "        # Resize and pad the image if convert_size is True\n",
    "        if self.target_image_size:\n",
    "            image = self.resize_and_pad(image, self.target_image_size, self.gamma)\n",
    "\n",
    "        # Extract the filename\n",
    "        # os.path.basename extract the base name (i.e., the final part) of a pathname\n",
    "        image_filename = os.path.basename(self.image_filenames[index])\n",
    "\n",
    "        return image, image_filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def resize_and_pad(self, image, target_image_size, is_mask=False):\n",
    "\n",
    "        gamma = self.gamma\n",
    "\n",
    "        \"\"\"Resize an image or mask and add padding to keep aspect ratio.\"\"\"\n",
    "        # Calculate scale and padding\n",
    "        h, w = image.shape[:2]\n",
    "        scale = min(target_image_size[0] / h, target_image_size[1] / w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        pad_h = (target_image_size[0] - new_h) // 2\n",
    "        pad_w = (target_image_size[1] - new_w) // 2\n",
    "\n",
    "        # Set parameters for resizing based on whether it's an image or a mask\n",
    "        if is_mask:\n",
    "            # Use nearest-neighbor interpolation for masks\n",
    "            order = 0\n",
    "            anti_aliasing = False\n",
    "        else:\n",
    "            # Use bilinear interpolation (order=1) for images\n",
    "            order = 1\n",
    "            anti_aliasing = True # minimize visual distortions when reducing the image size; averaging the colors of the pixels at the edges of contrasting areas (blurs edges)\n",
    "\n",
    "        # Resize image with preserve_range set to True\n",
    "        image_resized = transform.resize(\n",
    "            image,\n",
    "            (new_h, new_w),\n",
    "            anti_aliasing=anti_aliasing,\n",
    "            mode='constant',\n",
    "            preserve_range=True,  # Preserve the original image's intensity range\n",
    "            order=order\n",
    "        )\n",
    "\n",
    "        # Apply gamma correction to images only\n",
    "        if not is_mask and gamma != 1:\n",
    "            image_resized = exposure.adjust_gamma(image_resized, gamma)\n",
    "\n",
    "        # Add a channel dimension to grayscale images if necessary\n",
    "        if len(image.shape) == 2:  # Grayscale image\n",
    "            image_resized = image_resized[..., np.newaxis]\n",
    "\n",
    "        # Initialize padded image\n",
    "        padded_image = np.zeros((target_image_size[0], target_image_size[1], image_resized.shape[2]), dtype=image_resized.dtype)\n",
    "\n",
    "        # Insert the resized image into the padded image\n",
    "        padded_image[pad_h:pad_h+new_h, pad_w:pad_w+new_w, :] = image_resized\n",
    "\n",
    "        return padded_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab524659",
   "metadata": {
    "id": "yGM9oq-ddwr8",
    "papermill": {
     "duration": 0.018047,
     "end_time": "2024-02-06T22:59:51.829718",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.811671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Display Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d8f2d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:51.868033Z",
     "iopub.status.busy": "2024-02-06T22:59:51.867336Z",
     "iopub.status.idle": "2024-02-06T22:59:51.878247Z",
     "shell.execute_reply": "2024-02-06T22:59:51.877312Z"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1706587642020,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "H5lcJfZddwr9",
    "papermill": {
     "duration": 0.032418,
     "end_time": "2024-02-06T22:59:51.880206",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.847788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_image(images, titles=None, max_cols=4):\n",
    "    # Wrap single image in a list\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "        titles = [titles] if titles is not None else titles\n",
    "\n",
    "    if titles is not None and not isinstance(titles, list):\n",
    "        raise TypeError(\"Titles should be provided as a list or a single title for a single image.\")\n",
    "\n",
    "    if titles and len(images) != len(titles):\n",
    "        raise ValueError(\"Every image should have a corresponding title.\")\n",
    "\n",
    "    num_images = len(images)\n",
    "    cols = min(num_images, max_cols)\n",
    "    rows = num_images // cols + (num_images % cols > 0)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    if rows == 1 or cols == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    axes = axes.flatten()[:num_images]  # Limit to the number of images to avoid blank subplots\n",
    "\n",
    "    for ax, img, title in zip(axes, images, titles or [None]*num_images):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.cpu().numpy()\n",
    "            if img.ndim == 3 and img.shape[0] in [1, 3, 4]:\n",
    "                img = img.transpose(1, 2, 0)\n",
    "            if img.ndim == 3 and img.shape[2] == 1:\n",
    "                img = img.squeeze(2)\n",
    "\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42645b1",
   "metadata": {
    "id": "-wKhxrwOdwr-",
    "papermill": {
     "duration": 0.017502,
     "end_time": "2024-02-06T22:59:51.914800",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.897298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Load Dataset and Display Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dccfa7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:51.952145Z",
     "iopub.status.busy": "2024-02-06T22:59:51.951787Z",
     "iopub.status.idle": "2024-02-06T22:59:51.959389Z",
     "shell.execute_reply": "2024-02-06T22:59:51.958483Z"
    },
    "executionInfo": {
     "elapsed": 15431,
     "status": "ok",
     "timestamp": 1706587657449,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "6tvqBN8Fdwr-",
    "outputId": "2fd448fb-df96-4aef-d009-46302f365a04",
    "papermill": {
     "duration": 0.029012,
     "end_time": "2024-02-06T22:59:51.961753",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.932741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /kaggle/working\n",
      "Full image path: /kaggle/input/blood-vessel-segmentation/train/kidney_1_dense/images\n",
      "Full mask path: /kaggle/input/blood-vessel-segmentation/train/kidney_1_dense/labels\n",
      "Checkpoint path: /kaggle/working/checkpoints\n",
      "Model path: /kaggle/input/attention-unet/pytorch/real/1/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Get dataset paths\n",
    "image_path = os.path.join(CFG.TRAIN_DATASET_DIRECTORY, \"images\")\n",
    "mask_path = os.path.join(CFG.TRAIN_DATASET_DIRECTORY, \"labels\")\n",
    "\n",
    "# Print current working directory and full paths for debugging\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Full image path:\", os.path.abspath(image_path))\n",
    "print(\"Full mask path:\", os.path.abspath(mask_path))\n",
    "print(\"Checkpoint path:\", os.path.abspath(CFG.CHECKPOINT_PATH))\n",
    "if CFG.MODEL_PATH:\n",
    "    print(\"Model path:\", os.path.abspath(CFG.MODEL_PATH))\n",
    "\n",
    "if not CFG.IF_SUBMISSION: # Initialize dataset\n",
    "    dataset = CustomDataset(image_path, mask_path, target_image_size=CFG.INPUT_IMAGE_SIZE, gamma=CFG.GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ec5032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.000378Z",
     "iopub.status.busy": "2024-02-06T22:59:51.999999Z",
     "iopub.status.idle": "2024-02-06T22:59:52.005673Z",
     "shell.execute_reply": "2024-02-06T22:59:52.004675Z"
    },
    "executionInfo": {
     "elapsed": 4118,
     "status": "ok",
     "timestamp": 1706587661557,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "Uk-pfasrdwr_",
    "outputId": "e192e6c9-c212-4817-805b-708743c4fb61",
    "papermill": {
     "duration": 0.027703,
     "end_time": "2024-02-06T22:59:52.007727",
     "exception": false,
     "start_time": "2024-02-06T22:59:51.980024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not CFG.IF_SUBMISSION:\n",
    "    index = 600\n",
    "    original_image = io.imread(os.path.join(CFG.TRAIN_DATASET_DIRECTORY, \"images\", dataset[index][2]))\n",
    "    display_image([original_image, dataset[index][0], dataset[index][1]], [f\"Image {dataset[index][2]}\", \"Processed\", f\"Mask {dataset[index][2]}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c6a1c",
   "metadata": {
    "id": "F8Q6_o0bdwr_",
    "papermill": {
     "duration": 0.018439,
     "end_time": "2024-02-06T22:59:52.044595",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.026156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.4 Train-Test-Split and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d03c2134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.082586Z",
     "iopub.status.busy": "2024-02-06T22:59:52.082194Z",
     "iopub.status.idle": "2024-02-06T22:59:52.091715Z",
     "shell.execute_reply": "2024-02-06T22:59:52.090812Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1706587661557,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "C5hIxhdQdwr_",
    "outputId": "d0bffdd5-2754-4b03-8cc1-493de8a246db",
    "papermill": {
     "duration": 0.030647,
     "end_time": "2024-02-06T22:59:52.093655",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.063008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not CFG.IF_SUBMISSION: \n",
    "    # Slice dataset\n",
    "    if not CFG.NUM_IMAGES:\n",
    "        pass\n",
    "    elif CFG.NUM_IMAGES < 6:\n",
    "        raise(\"Number of samples is not sufficient. Raise number over 6.\")\n",
    "    else:\n",
    "        # Create a subset of the dataset\n",
    "        indices = torch.randperm(len(dataset)).tolist()\n",
    "        subset_indices = indices[:CFG.NUM_IMAGES]\n",
    "        subset_dataset = Subset(dataset, subset_indices)\n",
    "        dataset = subset_dataset\n",
    "\n",
    "    # Calculate the sizes of train and test sets\n",
    "    total_size = len(dataset)\n",
    "    test_size = int(total_size * CFG.TRAIN_TEST_SPLIT_RATIO)\n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    # Split the dataset into train and test datasets\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Further split the train dataset into train and validation datasets\n",
    "    val_size = int(train_size * CFG.TRAIN_TEST_SPLIT_RATIO)\n",
    "    new_train_size = train_size - val_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [new_train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders for train, validation, and test datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True, num_workers=CFG.NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=CFG.NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=CFG.NUM_WORKERS)\n",
    "\n",
    "    print(\"Numbers of batches in train, valid, test data loader are:\")\n",
    "    print(f\"Train: {len(train_loader)}, Validation: {len(val_loader)}, Test: {len(test_loader)}\")\n",
    "\n",
    "    print(\"Numbers of examples in train, valid, test data loader are:\")\n",
    "    print(f\"Train: {len(train_loader.sampler)}, Validation: {len(val_loader.sampler)}, Test: {len(test_loader.sampler)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811ea63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.132233Z",
     "iopub.status.busy": "2024-02-06T22:59:52.131834Z",
     "iopub.status.idle": "2024-02-06T22:59:52.137121Z",
     "shell.execute_reply": "2024-02-06T22:59:52.136146Z"
    },
    "executionInfo": {
     "elapsed": 2559,
     "status": "ok",
     "timestamp": 1706587664103,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "6BHBO-XdfKNX",
    "outputId": "116a76a9-a008-4304-845d-a199280db7dd",
    "papermill": {
     "duration": 0.027434,
     "end_time": "2024-02-06T22:59:52.139404",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.111970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not CFG.IF_SUBMISSION:\n",
    "    data, target, filename = next(iter(train_loader))\n",
    "    print(\"Shape of the data: \", data.shape)  # Expected: [batch_size, 1, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b8171f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.179967Z",
     "iopub.status.busy": "2024-02-06T22:59:52.179577Z",
     "iopub.status.idle": "2024-02-06T22:59:52.186786Z",
     "shell.execute_reply": "2024-02-06T22:59:52.185892Z"
    },
    "executionInfo": {
     "elapsed": 199614,
     "status": "ok",
     "timestamp": 1706587863712,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "kC4osoVtfyfj",
    "outputId": "fd74d612-a0c4-41a4-8054-2c7229331a50",
    "papermill": {
     "duration": 0.029977,
     "end_time": "2024-02-06T22:59:52.188903",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.158926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_target_masks(data_loader):\n",
    "    for batch_index, (data, target, filename) in enumerate(tqdm(data_loader)):\n",
    "        target_np = target.cpu().numpy()\n",
    "\n",
    "        if not np.all(np.isin(target_np, [0, 1])):\n",
    "            unique_values = np.unique(target_np)\n",
    "            return False, f\"Non-binary values found in batch {batch_index}. Unique values: {unique_values}\"\n",
    "\n",
    "    return True, \"All target masks are binary (contain only 0s and 1s)\"\n",
    "\n",
    "if CFG.BINARY_CHECK:\n",
    "  is_binary, message = check_target_masks(train_loader)\n",
    "  print(message)\n",
    "  is_binary, message = check_target_masks(val_loader)\n",
    "  print(message)\n",
    "  is_binary, message = check_target_masks(test_loader)\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426dc145",
   "metadata": {
    "id": "f453x8uXdwr_",
    "papermill": {
     "duration": 0.01845,
     "end_time": "2024-02-06T22:59:52.226427",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.207977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddac5187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.327812Z",
     "iopub.status.busy": "2024-02-06T22:59:52.327071Z",
     "iopub.status.idle": "2024-02-06T22:59:52.338499Z",
     "shell.execute_reply": "2024-02-06T22:59:52.337416Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1706587863713,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "3hEOOnAhdwr_",
    "papermill": {
     "duration": 0.033781,
     "end_time": "2024-02-06T22:59:52.340678",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.306897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augement_image(image, mask):\n",
    "\n",
    "    # convert np array to float type before converting into tensor\n",
    "    image_dense = torch.from_numpy(image.astype(np.float32))\n",
    "    mask_dense = torch.from_numpy(mask.astype(np.float32))\n",
    "\n",
    "    if len(image_dense.shape) == 2:\n",
    "        image_dense = image_dense.unsqueeze(0) # unsqueeze add new dimention at the specified index = 0 (at the beginning)\n",
    "\n",
    "    image_np = image_dense.permute(1, 2, 0).numpy() # permute to rearrange the dimensions of tensor\n",
    "    mask_np = mask_dense.numpy()\n",
    "\n",
    "    image_list = [None, None, None, None]\n",
    "    mask_list = [None, None, None, None]\n",
    "\n",
    "    transform_rotate_90 = Alb.Compose([\n",
    "        Alb.Rotate(limit=90, p=0.5)\n",
    "    ])\n",
    "\n",
    "    # Original image\n",
    "    image_list[0] = image_np\n",
    "    mask_list[0] = mask_np\n",
    "\n",
    "    # Original image with 90-degree rotation\n",
    "    augmented_rotate_90 = transform_rotate_90(image=image_np, mask=mask_np)\n",
    "    image_list[1], mask_list[1] = augmented_rotate_90['image'], augmented_rotate_90['mask']\n",
    "\n",
    "    # Original image with 180-degree rotation\n",
    "    augmented_rotate_180 = transform_rotate_90(image=image_list[1], mask=mask_list[1])\n",
    "    image_list[2], mask_list[2] = augmented_rotate_180['image'], augmented_rotate_180['mask']\n",
    "\n",
    "    # Original image with 270-degree rotation\n",
    "    augmented_rotate_270 = transform_rotate_90(image=image_list[2], mask=mask_list[2])\n",
    "    image_list[3], mask_list[3] = augmented_rotate_270['image'], augmented_rotate_270['mask']\n",
    "\n",
    "    # Making the mask dimensions 3\n",
    "    for i in range(4):\n",
    "        if mask_list[i].ndim == 2:\n",
    "            mask_list[i] = mask_list[i][..., np.newaxis]\n",
    "\n",
    "    augmented_data_zip = list(zip(*[image_list, mask_list]))\n",
    "\n",
    "    return augmented_data_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753e3fa",
   "metadata": {
    "id": "WFhooUQydwsA",
    "papermill": {
     "duration": 0.018276,
     "end_time": "2024-02-06T22:59:52.377877",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.359601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ad42bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.416621Z",
     "iopub.status.busy": "2024-02-06T22:59:52.416219Z",
     "iopub.status.idle": "2024-02-06T22:59:52.420819Z",
     "shell.execute_reply": "2024-02-06T22:59:52.419814Z"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1706587864110,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "ZbSSLr1GdwsA",
    "papermill": {
     "duration": 0.026459,
     "end_time": "2024-02-06T22:59:52.423009",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.396550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SSL certificate expired for se_resnext50_32x4d encoder\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362be82b",
   "metadata": {
    "id": "gr-OA-DEdwsA",
    "papermill": {
     "duration": 0.018237,
     "end_time": "2024-02-06T22:59:52.459899",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.441662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1 Attention U-Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b5db55c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.500889Z",
     "iopub.status.busy": "2024-02-06T22:59:52.500516Z",
     "iopub.status.idle": "2024-02-06T22:59:52.523281Z",
     "shell.execute_reply": "2024-02-06T22:59:52.522279Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1706587864110,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "dN7kOYbVdwsA",
    "papermill": {
     "duration": 0.046354,
     "end_time": "2024-02-06T22:59:52.525653",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.479299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    # filter/kernel slides over the input image and produce a feature map (3x3 or 5x5 pixels)\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # extracts features by applying filters\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "            # normalizes features, stablizees and speed up training\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            # introduces non-linearity, allows network to learn more complex patterns\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    # use convolutional and pooling layers to capture the context of the image\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        # extract feature masps\n",
    "        self.conv = ConvolutionalBlock(in_c, out_c)\n",
    "        # select most significant pixel value in each filter patch of the feature map\n",
    "        # down-sampels feature maps\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.conv(x)\n",
    "        p = self.pool(s)\n",
    "        return s, p\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    # useful when the area of interest occupies a relatively small part of the image\n",
    "    # suppress irrelevant regions in input while highlighting features useful for a specific task\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        # Wg = weights for gating signal; from a deeper layer (high-level features)\n",
    "        # gating signal helps to identify region of interest\n",
    "        self.Wg = nn.Sequential(\n",
    "            nn.Conv2d(in_c[0], out_c, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_c)\n",
    "        )\n",
    "\n",
    "        # Ws = weights ofr skip connection signal; from an earlier layer (low-level features)\n",
    "        # refirned skip connection signal ensures that the network retains crucial spatial details\n",
    "        self.Ws = nn.Sequential(\n",
    "            nn.Conv2d(in_c[1], out_c, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(out_c)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, g, s):\n",
    "        Wg = self.Wg(g)\n",
    "        Ws = self.Ws(s)\n",
    "        # integrate contextual info by Wg with spatial details from Ws and filter out negative activations\n",
    "        out = self.relu(Wg + Ws)\n",
    "        out = self.output(out)\n",
    "        return out * s\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    # use convolutional and up-sampling layers to enable precise localization\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        # increase spatial dimensions\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.ag = AttentionBlock(in_c, out_c)\n",
    "        self.c1 = ConvolutionalBlock(in_c[0]+out_c, out_c)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        x = self.up(x)\n",
    "        s = self.ag(x, s)\n",
    "        x = torch.cat([x, s], axis=1)\n",
    "        x = self.c1(x)\n",
    "        return x\n",
    "\n",
    "class attention_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Adusted for 1 input channel\n",
    "        self.e1 = EncoderBlock(1, 64)  # Adjusted for 1 input channel\n",
    "        self.e2 = EncoderBlock(64, 128)\n",
    "        self.e3 = EncoderBlock(128, 256)\n",
    "        # Add more encoder blocks if needed to accommodate the larger input size\n",
    "\n",
    "        self.b1 = ConvolutionalBlock(256, 512)\n",
    "\n",
    "        self.d1 = DecoderBlock([512, 256], 256)\n",
    "        self.d2 = DecoderBlock([256, 128], 128)\n",
    "        self.d3 = DecoderBlock([128, 64], 64)\n",
    "        # Add more decoder blocks symmetrically if more encoder blocks are added\n",
    "\n",
    "        self.output = nn.Conv2d(64, 1, kernel_size=1, padding=0)  # Output for 1 channel\n",
    "\n",
    "    def forward(self, x):\n",
    "        s1, p1 = self.e1(x)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "\n",
    "        b1 = self.b1(p3)\n",
    "\n",
    "        d1 = self.d1(b1, s3)\n",
    "        d2 = self.d2(d1, s2)\n",
    "        d3 = self.d3(d2, s1)\n",
    "\n",
    "        output = self.output(d3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9be6f",
   "metadata": {
    "id": "x9IfftSVdwsA",
    "papermill": {
     "duration": 0.019054,
     "end_time": "2024-02-06T22:59:52.563824",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.544770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2 Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b0beb9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:52.605371Z",
     "iopub.status.busy": "2024-02-06T22:59:52.604391Z",
     "iopub.status.idle": "2024-02-06T22:59:53.870602Z",
     "shell.execute_reply": "2024-02-06T22:59:53.869618Z"
    },
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1706587865185,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "9uF6C7DYdwsA",
    "outputId": "3995aba1-28e0-461d-8218-999494ec391b",
    "papermill": {
     "duration": 1.289463,
     "end_time": "2024-02-06T22:59:53.872669",
     "exception": false,
     "start_time": "2024-02-06T22:59:52.583206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "attention_unet                           [1, 1, 512, 512]          --\n",
       "├─EncoderBlock: 1-1                      [1, 64, 512, 512]         --\n",
       "│    └─ConvolutionalBlock: 2-1           [1, 64, 512, 512]         --\n",
       "│    │    └─Sequential: 3-1              [1, 64, 512, 512]         37,824\n",
       "│    └─MaxPool2d: 2-2                    [1, 64, 256, 256]         --\n",
       "├─EncoderBlock: 1-2                      [1, 128, 256, 256]        --\n",
       "│    └─ConvolutionalBlock: 2-3           [1, 128, 256, 256]        --\n",
       "│    │    └─Sequential: 3-2              [1, 128, 256, 256]        221,952\n",
       "│    └─MaxPool2d: 2-4                    [1, 128, 128, 128]        --\n",
       "├─EncoderBlock: 1-3                      [1, 256, 128, 128]        --\n",
       "│    └─ConvolutionalBlock: 2-5           [1, 256, 128, 128]        --\n",
       "│    │    └─Sequential: 3-3              [1, 256, 128, 128]        886,272\n",
       "│    └─MaxPool2d: 2-6                    [1, 256, 64, 64]          --\n",
       "├─ConvolutionalBlock: 1-4                [1, 512, 64, 64]          --\n",
       "│    └─Sequential: 2-7                   [1, 512, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-4                  [1, 512, 64, 64]          1,180,160\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 512, 64, 64]          1,024\n",
       "│    │    └─ReLU: 3-6                    [1, 512, 64, 64]          --\n",
       "│    │    └─Conv2d: 3-7                  [1, 512, 64, 64]          2,359,808\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 512, 64, 64]          1,024\n",
       "│    │    └─ReLU: 3-9                    [1, 512, 64, 64]          --\n",
       "├─DecoderBlock: 1-5                      [1, 256, 128, 128]        --\n",
       "│    └─Upsample: 2-8                     [1, 512, 128, 128]        --\n",
       "│    └─AttentionBlock: 2-9               [1, 256, 128, 128]        --\n",
       "│    │    └─Sequential: 3-10             [1, 256, 128, 128]        131,840\n",
       "│    │    └─Sequential: 3-11             [1, 256, 128, 128]        66,304\n",
       "│    │    └─ReLU: 3-12                   [1, 256, 128, 128]        --\n",
       "│    │    └─Sequential: 3-13             [1, 256, 128, 128]        65,792\n",
       "│    └─ConvolutionalBlock: 2-10          [1, 256, 128, 128]        --\n",
       "│    │    └─Sequential: 3-14             [1, 256, 128, 128]        2,360,832\n",
       "├─DecoderBlock: 1-6                      [1, 128, 256, 256]        --\n",
       "│    └─Upsample: 2-11                    [1, 256, 256, 256]        --\n",
       "│    └─AttentionBlock: 2-12              [1, 128, 256, 256]        --\n",
       "│    │    └─Sequential: 3-15             [1, 128, 256, 256]        33,152\n",
       "│    │    └─Sequential: 3-16             [1, 128, 256, 256]        16,768\n",
       "│    │    └─ReLU: 3-17                   [1, 128, 256, 256]        --\n",
       "│    │    └─Sequential: 3-18             [1, 128, 256, 256]        16,512\n",
       "│    └─ConvolutionalBlock: 2-13          [1, 128, 256, 256]        --\n",
       "│    │    └─Sequential: 3-19             [1, 128, 256, 256]        590,592\n",
       "├─DecoderBlock: 1-7                      [1, 64, 512, 512]         --\n",
       "│    └─Upsample: 2-14                    [1, 128, 512, 512]        --\n",
       "│    └─AttentionBlock: 2-15              [1, 64, 512, 512]         --\n",
       "│    │    └─Sequential: 3-20             [1, 64, 512, 512]         8,384\n",
       "│    │    └─Sequential: 3-21             [1, 64, 512, 512]         4,288\n",
       "│    │    └─ReLU: 3-22                   [1, 64, 512, 512]         --\n",
       "│    │    └─Sequential: 3-23             [1, 64, 512, 512]         4,160\n",
       "│    └─ConvolutionalBlock: 2-16          [1, 64, 512, 512]         --\n",
       "│    │    └─Sequential: 3-24             [1, 64, 512, 512]         147,840\n",
       "├─Conv2d: 1-8                            [1, 1, 512, 512]          65\n",
       "==========================================================================================\n",
       "Total params: 8,134,593\n",
       "Trainable params: 8,134,593\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 182.38\n",
       "==========================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 3122.66\n",
       "Params size (MB): 32.54\n",
       "Estimated Total Size (MB): 3156.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = attention_unet()\n",
    "# summary(model, input_size = (CFG.BATCH_SIZE, 1, CFG.INPUT_IMAGE_SIZE[0], CFG.INPUT_IMAGE_SIZE[1]))\n",
    "summary(model, input_size = (1, 1, CFG.INPUT_IMAGE_SIZE[0], CFG.INPUT_IMAGE_SIZE[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33841a",
   "metadata": {
    "id": "NBZYvp2hdwsB",
    "papermill": {
     "duration": 0.019023,
     "end_time": "2024-02-06T22:59:53.911797",
     "exception": false,
     "start_time": "2024-02-06T22:59:53.892774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67463cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:53.950362Z",
     "iopub.status.busy": "2024-02-06T22:59:53.950013Z",
     "iopub.status.idle": "2024-02-06T22:59:53.959724Z",
     "shell.execute_reply": "2024-02-06T22:59:53.958737Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706591572982,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "gsvl3lKWdwsB",
    "papermill": {
     "duration": 0.03154,
     "end_time": "2024-02-06T22:59:53.961832",
     "exception": false,
     "start_time": "2024-02-06T22:59:53.930292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    if rle == '':\n",
    "      rle = '1 0'\n",
    "    return rle\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a248f9",
   "metadata": {
    "id": "oKusBylNdwsB",
    "papermill": {
     "duration": 0.017633,
     "end_time": "2024-02-06T22:59:53.998045",
     "exception": false,
     "start_time": "2024-02-06T22:59:53.980412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.1 Run-Length Encode and Decode\n",
    "\n",
    "Run-Length Encoding (RLE) is a simple form of data compression where runs of data (i.e., sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count. It is particularly effective for data with many such runs, as it can significantly reduce the size of the data.\n",
    "\n",
    "In the context of image segmentation, such as in a Kaggle competition for blood vessel segmentation from CT images, RLE is often used to encode binary masks. In a binary mask, each pixel is either part of the object of interest (e.g., a blood vessel) or the background. RLE can efficiently represent these masks, especially when large regions of pixels are the same (all vessel or all background).\n",
    "\n",
    "The encoding typically works as follows:\n",
    "\n",
    "1. The binary mask is flattened into a one-dimensional array (e.g., by taking each row of pixels in turn).\n",
    "2. The algorithm goes through this array and counts the number of consecutive pixels with the same value.\n",
    "3. Each run of pixels is then represented by two numbers: the start position in the flattened array and the length of the run.\n",
    "\n",
    "For example, the array <code>[0, 0, 1, 1, 1, 0, 0]</code> would be encoded as <code>[2, 3]</code> in RLE, meaning that starting from the third element, there are three consecutive 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b84db",
   "metadata": {
    "id": "ALovvQhGdwsB",
    "papermill": {
     "duration": 0.017745,
     "end_time": "2024-02-06T22:59:54.034024",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.016279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.2 Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0e43b",
   "metadata": {
    "id": "KG5qQS4IdwsB",
    "papermill": {
     "duration": 0.019911,
     "end_time": "2024-02-06T22:59:54.073220",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.053309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.2.1. DICE Loss\n",
    "Dice loss is particularly useful for data with imbalanced classes. It measures the overlap between the predicted segmentation and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95701d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.114025Z",
     "iopub.status.busy": "2024-02-06T22:59:54.113113Z",
     "iopub.status.idle": "2024-02-06T22:59:54.119458Z",
     "shell.execute_reply": "2024-02-06T22:59:54.118465Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1706587865186,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "PWIDhMPGdwsB",
    "papermill": {
     "duration": 0.029205,
     "end_time": "2024-02-06T22:59:54.121634",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.092429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = inputs.sigmoid()\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2df44",
   "metadata": {
    "id": "gs0lchYWdwsB",
    "papermill": {
     "duration": 0.019458,
     "end_time": "2024-02-06T22:59:54.161267",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.141809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.2.2. Jaccard/Intersection over Union (IoU) Loss\n",
    "Similar to Dice loss, IoU is another common metric for the evaluation of object detection algorithms, such as image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4607ec18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.203857Z",
     "iopub.status.busy": "2024-02-06T22:59:54.203129Z",
     "iopub.status.idle": "2024-02-06T22:59:54.209874Z",
     "shell.execute_reply": "2024-02-06T22:59:54.208758Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1706587865186,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "_J3nwA_EdwsB",
    "papermill": {
     "duration": 0.030524,
     "end_time": "2024-02-06T22:59:54.212051",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.181527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IoULoss(nn.Module):\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = inputs.sigmoid()\n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection\n",
    "        IoU = (intersection + smooth) / (union + smooth)\n",
    "        return 1 - IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b964c",
   "metadata": {
    "id": "BN6TDd1VdwsB",
    "papermill": {
     "duration": 0.018959,
     "end_time": "2024-02-06T22:59:54.250673",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.231714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.2.3 Focal Loss\n",
    "This loss function is designed to address class imbalance by down-weighting well-classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0ff340c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.290415Z",
     "iopub.status.busy": "2024-02-06T22:59:54.289651Z",
     "iopub.status.idle": "2024-02-06T22:59:54.297817Z",
     "shell.execute_reply": "2024-02-06T22:59:54.296806Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1706587865186,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "pClaSPQ5dwsB",
    "papermill": {
     "duration": 0.03041,
     "end_time": "2024-02-06T22:59:54.300000",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.269590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b0bb88",
   "metadata": {
    "id": "lWnrPkcZdwsC",
    "papermill": {
     "duration": 0.018989,
     "end_time": "2024-02-06T22:59:54.338133",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.319144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.4 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeaa638f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.377689Z",
     "iopub.status.busy": "2024-02-06T22:59:54.377286Z",
     "iopub.status.idle": "2024-02-06T22:59:54.387759Z",
     "shell.execute_reply": "2024-02-06T22:59:54.386763Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1706587865186,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "05OC4_OpdwsC",
    "papermill": {
     "duration": 0.033115,
     "end_time": "2024-02-06T22:59:54.389984",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.356869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculates IOU, F1 score, precision, recall, and accuracy for U-Net predictions using NumPy.\n",
    "\n",
    "    Arguments:\n",
    "    y_true -- the ground truth labels (a binary array of 0s and 1s)\n",
    "    y_pred -- the predicted labels (a binary array of 0s and 1s)\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the IOU, F1 score, precision, recall, and accuracy.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert probability output to binary predictions\n",
    "        y_pred = (y_pred > 0.5).float()\n",
    "\n",
    "        # Flatten the arrays\n",
    "        y_true = y_true.detach().cpu().numpy().flatten()\n",
    "        y_pred = y_pred.detach().cpu().numpy().flatten()\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "        # Very small number to prevent infinity when dividing zero\n",
    "        eps = 1e-8\n",
    "\n",
    "        # Calculate IOU\n",
    "        iou = tp / (tp + fp + fn + eps)\n",
    "\n",
    "        # Calculate precision\n",
    "        precision = tp / (tp + fp + eps)\n",
    "\n",
    "        # Calculate recall\n",
    "        recall = tp / (tp + fn + eps)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + eps)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "    # Return a dictionary containing the metrics\n",
    "    metrics = {\"IoU_Score\": iou, \"F1_Score\": f1_score, \"Precision\": precision, \"Recall\": recall, \"Accuracy\": accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf846d",
   "metadata": {
    "id": "6DGAGBHodwsC",
    "papermill": {
     "duration": 0.018663,
     "end_time": "2024-02-06T22:59:54.428211",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.409548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5397e",
   "metadata": {
    "id": "gaVUZqgYdwsC",
    "papermill": {
     "duration": 0.01902,
     "end_time": "2024-02-06T22:59:54.466687",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.447667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6.1 Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0422eea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.509026Z",
     "iopub.status.busy": "2024-02-06T22:59:54.508602Z",
     "iopub.status.idle": "2024-02-06T22:59:54.539721Z",
     "shell.execute_reply": "2024-02-06T22:59:54.538650Z"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1706588269497,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "xkvPjijkdwsC",
    "papermill": {
     "duration": 0.055626,
     "end_time": "2024-02-06T22:59:54.542140",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.486514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, criterion, device, checkpoint_path, model_name):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.model_name = model_name\n",
    "        self.best_valid_loss = float('inf')\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_index, batch in enumerate(tqdm(self.train_loader)):\n",
    "            self.optimizer.zero_grad()\n",
    "            data, target, _ = batch\n",
    "\n",
    "            data = data.to(self.device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "            target = target.to(self.device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "            output = self.model(data)\n",
    "            train_loss = self.criterion(output, target)\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_train_loss += train_loss.item()\n",
    "\n",
    "        average_train_loss = total_train_loss / len(self.train_loader)\n",
    "        return average_train_loss\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, batch in enumerate(tqdm(self.val_loader)):\n",
    "                data, target, _ = batch\n",
    "                data = data.to(self.device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "                target = target.to(self.device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "                output = self.model(data)\n",
    "                val_loss = self.criterion(output, target).item()\n",
    "                total_val_loss += val_loss\n",
    "\n",
    "        average_val_loss = total_val_loss / len(self.val_loader)\n",
    "        return average_val_loss\n",
    "\n",
    "    def train(self, epochs, resume_from_path=None):\n",
    "        # Check if the checkpoint directory exists\n",
    "        if not os.path.exists(self.checkpoint_path):\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "            print(f\"Checkpoint directory created at {self.checkpoint_path}\")\n",
    "        else:\n",
    "            print(f\"Checkpoint directory already exists at {self.checkpoint_path}\")\n",
    "        \n",
    "        training_log_path = os.path.join(self.checkpoint_path, f\"{self.model_name}_train_results.csv\")\n",
    "        \n",
    "        # Resume training if true\n",
    "        if resume_from_path:\n",
    "            print(f\"Resuming training from checkpoint: {resume_from_path}\")\n",
    "            start_epoch, train_df = self.load_model(resume_from_path)\n",
    "        else:\n",
    "            print(\"Starting training from scratch\")\n",
    "            start_epoch = 0\n",
    "            train_df = pd.DataFrame(columns=[\"Epoch\", \"Train_Loss\", \"Valid_Loss\", \"Time_Per_Iteration\"])\n",
    "        \n",
    "        # Training iteration\n",
    "        self.save_config(CFG)\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch # {start_epoch + epoch + 1}\")\n",
    "            start_time = time.time()\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss = self.validate()\n",
    "            iteration_time = (time.time() - start_time) / len(self.train_loader)\n",
    "\n",
    "            result_df = pd.DataFrame({\n",
    "                \"Epoch\": [start_epoch + epoch + 1],\n",
    "                \"Train_Loss\": [train_loss],\n",
    "                \"Valid_Loss\": [val_loss],\n",
    "                \"Time_Per_Iteration\": [iteration_time]\n",
    "            })\n",
    "            train_df = pd.concat([train_df, result_df], ignore_index=True)\n",
    "\n",
    "            train_df.to_csv(training_log_path)\n",
    "\n",
    "            if val_loss < self.best_valid_loss:\n",
    "                print(f\"Improved Validation Loss from {self.best_valid_loss:.6f} to {val_loss:.6f}\")\n",
    "                self.best_valid_loss = val_loss\n",
    "                self.save_model(self.model, self.optimizer, (start_epoch + epoch + 1), self.best_valid_loss, self.checkpoint_path, self.model_name)\n",
    "                print(f\"Saved model at: {os.path.join(self.checkpoint_path, self.model_name)}\")\n",
    "            else:\n",
    "                print(f\"Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        return self.model, train_df\n",
    "    \n",
    "    def load_model(self, resume_from_path):\n",
    "        model_path = os.path.join(resume_from_path, self.model_name)\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise FileNotFoundError(f\"No trained model has been found at {model_path}.\")\n",
    "\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.best_valid_loss = checkpoint.get('best_valid_loss', float('inf'))\n",
    "\n",
    "        training_log_path = os.path.join(resume_from_path, f\"{self.model_name}_train_results.csv\")\n",
    "        if os.path.isfile(training_log_path):\n",
    "            train_df = pd.read_csv(training_log_path, index_col=0)\n",
    "            start_epoch = train_df['Epoch'].iloc[-1]\n",
    "            print(f\"Model states loaded, training will resume from epoch # {start_epoch}.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No training results have been found at {training_log_path}.\")\n",
    "\n",
    "        return start_epoch, train_df\n",
    "\n",
    "    def save_model(self, model, optimizer, epoch, best_valid_loss, path, filename):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_valid_loss': best_valid_loss\n",
    "        }, os.path.join(path, filename))\n",
    "\n",
    "    def save_config(self, config):\n",
    "        config_filename = f\"{self.model_name}_config.txt\"\n",
    "        config_path = os.path.join(self.checkpoint_path, config_filename)\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        with open(config_path, 'w') as file:\n",
    "            file.write(f\"Config Saved on: {current_time}\\n\\n\")\n",
    "            for attribute in dir(config):\n",
    "                if not attribute.startswith(\"__\") and not callable(getattr(config, attribute)):\n",
    "                    file.write(f\"{attribute}: {getattr(config, attribute)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278856bc",
   "metadata": {
    "id": "CfvQMOZYdwsC",
    "papermill": {
     "duration": 0.018876,
     "end_time": "2024-02-06T22:59:54.580761",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.561885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6.2 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72eec4d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.620055Z",
     "iopub.status.busy": "2024-02-06T22:59:54.619698Z",
     "iopub.status.idle": "2024-02-06T22:59:54.625912Z",
     "shell.execute_reply": "2024-02-06T22:59:54.625003Z"
    },
    "executionInfo": {
     "elapsed": 1578767,
     "status": "ok",
     "timestamp": 1706593199465,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "kmwop5TCdwsG",
    "outputId": "1f7a1b99-28a5-4924-f66e-bd01f93b1133",
    "papermill": {
     "duration": 0.02824,
     "end_time": "2024-02-06T22:59:54.627842",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.599602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.IF_TRAIN:\n",
    "    # Initialize trainer class\n",
    "    trainer = Trainer(model,\n",
    "                    train_loader,\n",
    "                    val_loader,\n",
    "                    optimizer=optim.Adam(model.parameters(),lr=CFG.LEARNING_RATE),\n",
    "                    criterion=DiceLoss(),\n",
    "                    device=CFG.DEVICE,\n",
    "                    checkpoint_path=CFG.CHECKPOINT_PATH,\n",
    "                    model_name=CFG.TRAINED_MODEL\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"{CFG.TRAIN_DATASET_NAME} is in training.\")\n",
    "    model, train_df = trainer.train(\n",
    "        epochs=CFG.NUM_EPOCHS,\n",
    "        resume_from_path=CFG.MODEL_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17904695",
   "metadata": {
    "id": "0uR5qXLMgFMT",
    "papermill": {
     "duration": 0.017853,
     "end_time": "2024-02-06T22:59:54.663628",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.645775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6.3 Results in Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56d1a0fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.699959Z",
     "iopub.status.busy": "2024-02-06T22:59:54.699620Z",
     "iopub.status.idle": "2024-02-06T22:59:54.705266Z",
     "shell.execute_reply": "2024-02-06T22:59:54.704478Z"
    },
    "executionInfo": {
     "elapsed": 1110,
     "status": "ok",
     "timestamp": 1706593666927,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "FxlNwWZMgIDS",
    "outputId": "720c88d5-8bb7-4088-c010-f405864bcc3a",
    "papermill": {
     "duration": 0.026001,
     "end_time": "2024-02-06T22:59:54.707179",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.681178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.IF_TRAIN:\n",
    "    # Load training results\n",
    "    train_df = pd.read_csv(os.path.join(CFG.CHECKPOINT_PATH, f\"{CFG.TRAINED_MODEL}_train_results.csv\"))\n",
    "\n",
    "    # Show line plots of loss values\n",
    "    sns.set_theme()\n",
    "    ax = sns.lineplot(data=train_df, x=\"Epoch\", y=\"Train_Loss\", label=\"Train Loss\")\n",
    "    ax = sns.lineplot(data=train_df, x=\"Epoch\", y=\"Valid_Loss\", label=\"Valid Loss\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch Numbers\")\n",
    "    ax.set_ylabel(\"Losses\")\n",
    "    ax.set_title(\"Valid and Train Losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadc92e",
   "metadata": {
    "id": "4w2LYjZ9dwsG",
    "papermill": {
     "duration": 0.021468,
     "end_time": "2024-02-06T22:59:54.749476",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.728008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02a403",
   "metadata": {
    "id": "NiokTalrpHUA",
    "papermill": {
     "duration": 0.025039,
     "end_time": "2024-02-06T22:59:54.797141",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.772102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.1 Tester Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e3623fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.840181Z",
     "iopub.status.busy": "2024-02-06T22:59:54.839351Z",
     "iopub.status.idle": "2024-02-06T22:59:54.866564Z",
     "shell.execute_reply": "2024-02-06T22:59:54.865516Z"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1706593738413,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "-z9PxPuBdwsG",
    "papermill": {
     "duration": 0.050807,
     "end_time": "2024-02-06T22:59:54.869720",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.818913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self, model, test_loader, criterion, device, checkpoint_path, model_name):\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.model_name = model_name\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Load model state dictionary\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        model_path = os.path.join(self.checkpoint_path, self.model_name)\n",
    "        if os.path.isfile(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            if \"model_state_dict\" in checkpoint:\n",
    "                self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "                print(f\"Model state loaded from {model_path}\")\n",
    "            else:\n",
    "                print(\"The checkpoint does not contain a model state dictionary.\")\n",
    "        else:\n",
    "            print(\"Model file not found. Please check the path and model_name.\")\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        test_df = pd.DataFrame(columns=[\"Batch\", \"Filename\", \"Loss\", \"Time_Per_Iteration\", \"IoU_Score\", \"F1_Score\", \"Precision\", \"Recall\", \"Accuracy\", \"rle\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (data, target, filenames) in enumerate(tqdm(self.test_loader)):\n",
    "                data = data.to(self.device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "                target = target.to(self.device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "                batch_start_time = time.time()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target).item()\n",
    "                test_loss += loss\n",
    "\n",
    "                batch_time = time.time() - batch_start_time\n",
    "\n",
    "                # Calculate and store metrics\n",
    "                metrics = self.calculate_metrics(output, target)\n",
    "\n",
    "                for i in range(data.size(0)):\n",
    "                    # Process each image in the batch for RLE encoding\n",
    "                    pred_mask = output[i].cpu().numpy().squeeze()  # Assuming binary mask output\n",
    "                    pred_mask = (pred_mask > 0.5).astype(np.uint8)  # Thresholding\n",
    "\n",
    "                    # RLE encode the mask\n",
    "                    rle_encoded = self.rle_encode(pred_mask)\n",
    "\n",
    "                    # Extract the first element from filename if it's a tuple\n",
    "                    filename_str = filenames[i][0] if isinstance(filenames[i], tuple) else filenames[i]\n",
    "\n",
    "                    # Append all information to test_df\n",
    "                    batch_df = pd.DataFrame({\n",
    "                        \"Batch\": [batch_index + 1],\n",
    "                        \"Filename\": [filename_str],\n",
    "                        \"Loss\": [loss],\n",
    "                        \"Time_Per_Iteration\": [batch_time],\n",
    "                        \"IoU_Score\": [metrics[\"IoU_Score\"]],\n",
    "                        \"F1_Score\": [metrics[\"F1_Score\"]],\n",
    "                        \"Precision\": [metrics[\"Precision\"]],\n",
    "                        \"Recall\": [metrics[\"Recall\"]],\n",
    "                        \"Accuracy\": [metrics[\"Accuracy\"]],\n",
    "                        \"rle\": [rle_encoded]\n",
    "                    })\n",
    "                    test_df = pd.concat([test_df, batch_df], ignore_index=True)\n",
    "\n",
    "        test_loss /= len(self.test_loader)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "        # Save test_df with all information\n",
    "        test_df.to_csv(os.path.join(self.checkpoint_path, f\"{self.model_name}_test_results.csv\"), index=False)\n",
    "\n",
    "        return self.model, test_df\n",
    "\n",
    "    def rle_encode(self, img):\n",
    "        '''\n",
    "        img: numpy array, 1 - mask, 0 - background\n",
    "        Returns run length as string formated\n",
    "        '''\n",
    "        pixels = img.flatten()\n",
    "        pixels = np.concatenate([[0], pixels, [0]])\n",
    "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "        runs[1::2] -= runs[::2]\n",
    "        rle = ' '.join(str(x) for x in runs)\n",
    "        if rle == '':\n",
    "          rle = '1 0'\n",
    "        return rle\n",
    "\n",
    "    def calculate_metrics(self, y_pred, y_true):\n",
    "        with torch.no_grad():\n",
    "            y_pred = (y_pred > 0.5).float()\n",
    "            y_true = y_true.view_as(y_pred)\n",
    "\n",
    "            y_true = y_true.detach().cpu().numpy().flatten()\n",
    "            y_pred = y_pred.detach().cpu().numpy().flatten()\n",
    "\n",
    "            tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "            fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "            fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "            eps = 1e-8\n",
    "\n",
    "            iou = tp / (tp + fp + fn + eps)\n",
    "            precision = tp / (tp + fp + eps)\n",
    "            recall = tp / (tp + fn + eps)\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall + eps)\n",
    "            accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "            return {\n",
    "                \"IoU_Score\": iou,\n",
    "                \"F1_Score\": f1_score,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"Accuracy\": accuracy\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4123946",
   "metadata": {
    "id": "ec_UiObnpTh7",
    "papermill": {
     "duration": 0.019679,
     "end_time": "2024-02-06T22:59:54.914012",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.894333",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.2 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f946683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:54.953520Z",
     "iopub.status.busy": "2024-02-06T22:59:54.953124Z",
     "iopub.status.idle": "2024-02-06T22:59:54.958819Z",
     "shell.execute_reply": "2024-02-06T22:59:54.957699Z"
    },
    "executionInfo": {
     "elapsed": 18224,
     "status": "ok",
     "timestamp": 1706593758834,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "L2lPThQzlQV6",
    "outputId": "24700b91-350a-487f-ab0c-506094c2fbd0",
    "papermill": {
     "duration": 0.028248,
     "end_time": "2024-02-06T22:59:54.961090",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.932842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.IF_TEST:\n",
    "    # Initialize tester class\n",
    "    tester = Tester(model=attention_unet(),\n",
    "                    test_loader=test_loader,\n",
    "                    criterion=DiceLoss(),\n",
    "                    device=CFG.DEVICE,\n",
    "                    checkpoint_path = CFG.CHECKPOINT_PATH,\n",
    "                    model_name=CFG.TRAINED_MODEL\n",
    "    )\n",
    "\n",
    "    # Test the model\n",
    "    model, test_df = tester.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e0f8c",
   "metadata": {
    "id": "9v3COcjepZf6",
    "papermill": {
     "duration": 0.01799,
     "end_time": "2024-02-06T22:59:55.001297",
     "exception": false,
     "start_time": "2024-02-06T22:59:54.983307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.3 Results in Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "430c33a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:55.043245Z",
     "iopub.status.busy": "2024-02-06T22:59:55.042341Z",
     "iopub.status.idle": "2024-02-06T22:59:55.052697Z",
     "shell.execute_reply": "2024-02-06T22:59:55.051620Z"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1706593775023,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "qrqqMMnbo8XD",
    "outputId": "ed1bce9f-04a3-4bf7-8c4d-45e592fe09b8",
    "papermill": {
     "duration": 0.035245,
     "end_time": "2024-02-06T22:59:55.054879",
     "exception": false,
     "start_time": "2024-02-06T22:59:55.019634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " if CFG.IF_TEST:\n",
    "    # Load test results\n",
    "    test_df = pd.read_csv(os.path.join(CFG.CHECKPOINT_PATH, f\"{CFG.TRAINED_MODEL}_test_results.csv\"))\n",
    "\n",
    "    # Calculate the average of each metric\n",
    "    average_scores = test_df[[\"IoU_Score\", \"F1_Score\", \"Precision\", \"Recall\", \"Accuracy\"]].mean()\n",
    "\n",
    "    # Print the averaged results of scores\n",
    "    print(\"Averaged Results of Scores:\")\n",
    "    print(average_scores)\n",
    "\n",
    "    # Calculate and print the average and total time per iteration\n",
    "    average_time_per_iteration = test_df[\"Time_Per_Iteration\"].mean()\n",
    "    total_time = test_df[\"Time_Per_Iteration\"].sum()\n",
    "\n",
    "    print(\"\\nAverage Time Per Iteration: {:.4f} seconds\".format(average_time_per_iteration))\n",
    "    print(\"Total Time for All Iterations: {:.4f} seconds\".format(total_time))\n",
    "\n",
    "    # Divide required items from the dataset to show them only\n",
    "    melted_data = test_df.melt(id_vars=[\"Filename\", \"Time_Per_Iteration\"],\n",
    "                               value_vars=[\"IoU_Score\", \"F1_Score\", \"Precision\", \"Recall\", \"Accuracy\"],\n",
    "                               var_name=\"Types of Metrics\", value_name=\"Score\")\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.violinplot(x=\"Types of Metrics\", y=\"Score\", data=melted_data,\n",
    "                   inner=\"quartile\",\n",
    "                   order=[\"IoU_Score\", \"F1_Score\", \"Precision\", \"Recall\", \"Accuracy\"])\n",
    "\n",
    "    # Remove side boundaries\n",
    "    sns.despine(left=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5a929",
   "metadata": {
    "id": "k0ieneaipg1A",
    "papermill": {
     "duration": 0.019504,
     "end_time": "2024-02-06T22:59:55.094067",
     "exception": false,
     "start_time": "2024-02-06T22:59:55.074563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.4 Illustrate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf05080c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:55.140979Z",
     "iopub.status.busy": "2024-02-06T22:59:55.140151Z",
     "iopub.status.idle": "2024-02-06T22:59:55.151268Z",
     "shell.execute_reply": "2024-02-06T22:59:55.149819Z"
    },
    "executionInfo": {
     "elapsed": 1517,
     "status": "ok",
     "timestamp": 1706593857828,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "_LZZi91epmN_",
    "outputId": "f782cf26-5534-4a92-d9c2-8ce7de99e8be",
    "papermill": {
     "duration": 0.037198,
     "end_time": "2024-02-06T22:59:55.154329",
     "exception": false,
     "start_time": "2024-02-06T22:59:55.117131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.IF_TEST:\n",
    "    # Choose index\n",
    "    index = 0\n",
    "    if index < 0 or index >= len(test_df):\n",
    "        raise ValueError(\"Index out of range.\")\n",
    "\n",
    "    # Extract data\n",
    "    data_row = test_df.iloc[index]\n",
    "    filename = data_row[\"Filename\"]\n",
    "    rle = data_row['rle']\n",
    "\n",
    "    # Get images\n",
    "    original_image = io.imread(os.path.join(CFG.TRAIN_DATASET_DIRECTORY, \"images\", filename))\n",
    "    label_image = io.imread(os.path.join(CFG.TRAIN_DATASET_DIRECTORY, \"labels\", filename))\n",
    "\n",
    "    # Check if RLE data exists and is valid\n",
    "    if pd.isna(rle):\n",
    "        print(f\"Missing RLE data at index {index}. Filename: {filename}\")\n",
    "        images = [original_image, label_image]\n",
    "        titles = [filename, \"Label\"]\n",
    "    else:\n",
    "        predicted_mask = rle_decode(rle, CFG.INPUT_IMAGE_SIZE)\n",
    "        images = [original_image, label_image, predicted_mask]\n",
    "        titles = [filename, \"Label\", \"Predicted Mask\"]\n",
    "\n",
    "    # Use the display function\n",
    "    display_image(images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7ed5f",
   "metadata": {
    "id": "Q7aJwwzm3lTr",
    "papermill": {
     "duration": 0.020997,
     "end_time": "2024-02-06T22:59:55.199848",
     "exception": false,
     "start_time": "2024-02-06T22:59:55.178851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "159bcce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:55.241248Z",
     "iopub.status.busy": "2024-02-06T22:59:55.240872Z",
     "iopub.status.idle": "2024-02-06T22:59:55.249202Z",
     "shell.execute_reply": "2024-02-06T22:59:55.248191Z"
    },
    "papermill": {
     "duration": 0.031217,
     "end_time": "2024-02-06T22:59:55.251312",
     "exception": false,
     "start_time": "2024-02-06T22:59:55.220095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def revert_resize_and_pad(mask_pred, original_image_size, target_image_size):\n",
    "    # Assuming mask_pred is a 2D array for a single-channel grayscale image\n",
    "    # Adjustments are needed if mask_pred comes with an extra singleton dimension\n",
    "    \n",
    "    # Ensure mask_pred is 2D (height, width) by removing any singleton dimensions\n",
    "    if mask_pred.ndim > 2:\n",
    "        mask_pred = mask_pred.squeeze()\n",
    "\n",
    "    original_height, original_width = original_image_size[:2]\n",
    "    \n",
    "    # Calculate the scale and padding used during the initial resize\n",
    "    scale = min(target_image_size[0] / original_height, target_image_size[1] / original_width)\n",
    "    resized_height, resized_width = int(original_height * scale), int(original_width * scale)\n",
    "    \n",
    "    pad_h = (target_image_size[0] - resized_height) // 2\n",
    "    pad_w = (target_image_size[1] - resized_width) // 2\n",
    "\n",
    "    # Crop the padded area from the mask\n",
    "    cropped_mask = mask_pred[pad_h:pad_h+resized_height, pad_w:pad_w+resized_width]\n",
    "\n",
    "    # Resize the cropped mask back to the original image size\n",
    "    reverted_mask = transform.resize(\n",
    "        cropped_mask,\n",
    "        (original_height, original_width),\n",
    "        order=0,\n",
    "        preserve_range=True,\n",
    "        anti_aliasing=False\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    return reverted_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e06fe12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:55.293201Z",
     "iopub.status.busy": "2024-02-06T22:59:55.292224Z",
     "iopub.status.idle": "2024-02-06T22:59:55.309424Z",
     "shell.execute_reply": "2024-02-06T22:59:55.308496Z"
    },
    "papermill": {
     "duration": 0.040584,
     "end_time": "2024-02-06T22:59:55.311731",
     "exception": false,
     "start_time": "2024-02-06T22:59:55.271147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_output(model, model_path, test_dataset, device, dataset_name):\n",
    "    if os.path.isfile(model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        if \"model_state_dict\" in checkpoint:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            model.to(device)\n",
    "            print(f\"Model state loaded from {model_path}\")\n",
    "        else:\n",
    "            raise(\"The checkpoint does not contain a model state dictionary.\")\n",
    "    else:\n",
    "        raise(\"Model file not found. Please check the path and model_name.\")\n",
    "        \n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    submission_list = []\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    for images, filenames in tqdm(DataLoader(test_dataset, batch_size=1)):\n",
    "        images = images.to(device, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "        # Initialize variables for storing ensemble predictions\n",
    "        ensemble_pred = torch.zeros_like(images, device=device)\n",
    "\n",
    "        # Process each axis for the weighted ensemble\n",
    "        for axis in range(3):\n",
    "            if axis > 0:\n",
    "                # Rotate images if axis is 1 or 2\n",
    "                images = torch.rot90(images, 1, [2, 3])\n",
    "\n",
    "            # Forward pass through the model\n",
    "            with torch.no_grad():\n",
    "                preds = model(images)\n",
    "                \n",
    "            # If axis was rotated, rotate predictions back to original orientation\n",
    "            if axis > 0:\n",
    "                preds = torch.rot90(preds, -1, [2, 3])\n",
    "\n",
    "            # Aggregate predictions\n",
    "            ensemble_pred += preds / 3\n",
    "\n",
    "        # Apply threshold to the aggregated predictions\n",
    "        threshold = CFG.THRESHOLD_PERCENTILE\n",
    "        mask_pred = (ensemble_pred > threshold).cpu().numpy().astype(np.uint8)\n",
    "        #print(f\"Converted: {mask_pred.shape}\")\n",
    "        mask_pred = revert_resize_and_pad(mask_pred, test_dataset.original_image_size, test_dataset.target_image_size)\n",
    "        #print(f\"Reverted: {mask_pred.shape}\")\n",
    "        mask_pred = np.squeeze(mask_pred)\n",
    "\n",
    "        # RLE encode the mask\n",
    "        rle = rle_encode(mask_pred)\n",
    "        \n",
    "        # ID\n",
    "        base_filename = os.path.splitext(os.path.basename(filenames[0]))[0]\n",
    "        identifier = f\"{dataset_name}_{base_filename}\"\n",
    "        \n",
    "        submission_list.append({\"id\": identifier, \"rle\": rle})\n",
    "\n",
    "    # Convert submission list to DataFrame and sort\n",
    "    submission_df = pd.DataFrame(submission_list)\n",
    "    submission_df = submission_df.sort_values(by='id')\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2c5afb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T22:59:55.351645Z",
     "iopub.status.busy": "2024-02-06T22:59:55.351263Z",
     "iopub.status.idle": "2024-02-06T23:13:10.296073Z",
     "shell.execute_reply": "2024-02-06T23:13:10.295090Z"
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1706593863555,
     "user": {
      "displayName": "Jennifer",
      "userId": "15059307469137133547"
     },
     "user_tz": 420
    },
    "id": "9I7FmT5bw6LO",
    "papermill": {
     "duration": 794.967393,
     "end_time": "2024-02-06T23:13:10.298281",
     "exception": false,
     "start_time": "2024-02-06T22:59:55.330888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/blood-vessel-segmentation/train/kidney_2/images']\n",
      "Original size: (1041, 1511), Converted size: (512, 512, 1)\n",
      "Model state loaded from /kaggle/input/attention-unet/pytorch/real/1/checkpoints/AUNet_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2217/2217 [13:13<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Dataframe has been created\n"
     ]
    }
   ],
   "source": [
    "if CFG.IF_SUBMISSION:\n",
    "    # Use glob to create a list of image directories within the 'test' directory\n",
    "    #image_dirs = glob(os.path.join(CFG.DATASET_DIRECTORY, \"test\", \"*\", \"images\"))\n",
    "    image_dirs = [os.path.join(CFG.DATASET_DIRECTORY, \"train\", CFG.TEST_DATASET_NAME, \"images\")]\n",
    "    print(image_dirs)\n",
    "    \n",
    "    # Initialize an empty list to store individual submission DataFrames\n",
    "    submission_dfs = []\n",
    "\n",
    "    for image_dir in image_dirs:\n",
    "        # Get all image file paths from the current directory\n",
    "        images = glob(os.path.join(image_dir, \"*.tif\"))\n",
    "\n",
    "        # Load dataset\n",
    "        dataset = PipelineDataset(images, target_image_size=CFG.INPUT_IMAGE_SIZE, gamma=CFG.GAMMA)\n",
    "\n",
    "        # Get the output in the desired format\n",
    "        current_df = get_output(\n",
    "            attention_unet(),\n",
    "            os.path.join(CFG.MODEL_PATH, CFG.TRAINED_MODEL),\n",
    "            dataset,\n",
    "            CFG.DEVICE,\n",
    "            os.path.basename(os.path.dirname(image_dir)),\n",
    "        )\n",
    "\n",
    "        # Append the current DataFrame to the list of submission DataFrames\n",
    "        submission_dfs.append(current_df)\n",
    "\n",
    "    # Concatenate all individual submission DataFrames into a single DataFrame\n",
    "    submission_df = pd.concat(submission_dfs)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission Dataframe has been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "325f609b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T23:13:10.766402Z",
     "iopub.status.busy": "2024-02-06T23:13:10.765599Z",
     "iopub.status.idle": "2024-02-06T23:13:10.780302Z",
     "shell.execute_reply": "2024-02-06T23:13:10.779236Z"
    },
    "papermill": {
     "duration": 0.221805,
     "end_time": "2024-02-06T23:13:10.782541",
     "exception": false,
     "start_time": "2024-02-06T23:13:10.560736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>kidney_2_0000</td>\n",
       "      <td>1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>kidney_2_0001</td>\n",
       "      <td>1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>kidney_2_0002</td>\n",
       "      <td>1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>kidney_2_0003</td>\n",
       "      <td>1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>kidney_2_0004</td>\n",
       "      <td>1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>kidney_2_0005</td>\n",
       "      <td>89607 3 91118 3 92629 3 336656 3 338167 3 3396...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                rle\n",
       "1362  kidney_2_0000                                                1 0\n",
       "877   kidney_2_0001                                                1 0\n",
       "1163  kidney_2_0002                                                1 0\n",
       "2077  kidney_2_0003                                                1 0\n",
       "1884  kidney_2_0004                                                1 0\n",
       "768   kidney_2_0005  89607 3 91118 3 92629 3 336656 3 338167 3 3396..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be0d2f",
   "metadata": {
    "papermill": {
     "duration": 0.191897,
     "end_time": "2024-02-06T23:13:11.171402",
     "exception": false,
     "start_time": "2024-02-06T23:13:10.979505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6962461,
     "sourceId": 61446,
     "sourceType": "competition"
    },
    {
     "datasetId": 4365548,
     "sourceId": 7515369,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 6276,
     "sourceId": 7972,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 813.913997,
   "end_time": "2024-02-06T23:13:13.629594",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-06T22:59:39.715597",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
